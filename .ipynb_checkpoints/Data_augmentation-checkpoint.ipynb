{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c0151c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/images/data_augmentation?hl=ko\n",
    "import matplotlib.pyplot as pit\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff062d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n",
      "(60000, 28, 28, 1)\n",
      "(10000, 28, 28, 1)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.0, shape=(), dtype=float32)\n",
      "tf.Tensor(0.30810735, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "pit.imshow(x_train[0], cmap='gray', vmin=0, vmax=1)\n",
    "\n",
    "x_train = x_train[..., tf.newaxis].astype('float32')\n",
    "x_test = x_test[..., tf.newaxis].astype('float32')\n",
    "# 1. NHW 데이터를 NHWC 데이터로 변경 2. float32 data type으로 변경...\n",
    "# N Number of samples. H Height. W width. C channel\n",
    "# NCHW로 사용 가능하지만, NHWC가 주요 모양.\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(tf.math.reduce_max(x_train))\n",
    "print(tf.math.reduce_min(x_train))\n",
    "print(tf.math.reduce_std(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d44947c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x1000 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_augumentation = tf.keras.Sequential([\n",
    "    # tf.keras.layers.RandomRotation(0.05),\n",
    "    tf.keras.layers.GaussianNoise(0.2), #std=?인 gaussian noise 추가 평균=0 default\n",
    "])\n",
    "\n",
    "#GaussianNoise는 training 할때만 작동함.\n",
    "aug_img = data_augumentation(x_train[0], training = True)\n",
    "\n",
    "pit.figure(figsize=(10,10))\n",
    "for i in range(9):\n",
    "    aug_img = data_augumentation(x_train[0], training = True)\n",
    "    aug_img = tf.reshape(aug_img, shape = (28, -1)) # 1x28x28x1 => 28x28\n",
    "    ax = pit.subplot(3,3, i+1) # 3x3 display 중에 i+1 block\n",
    "    pit.imshow(aug_img, cmap='gray', vmin=0, vmax=1)\n",
    "    pit.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0ad853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples\n",
      "Epoch 1/512\n",
      "32/32 [==============================] - 0s 9ms/sample - loss: 2.4659 - accuracy: 0.1250\n",
      "Epoch 2/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 2.3277 - accuracy: 0.1562\n",
      "Epoch 3/512\n",
      "32/32 [==============================] - 0s 947us/sample - loss: 2.2145 - accuracy: 0.1875\n",
      "Epoch 4/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 2.1136 - accuracy: 0.2188\n",
      "Epoch 5/512\n",
      "32/32 [==============================] - 0s 910us/sample - loss: 2.0496 - accuracy: 0.2188\n",
      "Epoch 6/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 1.9358 - accuracy: 0.2500\n",
      "Epoch 7/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 1.9085 - accuracy: 0.4062\n",
      "Epoch 8/512\n",
      "32/32 [==============================] - 0s 842us/sample - loss: 1.7926 - accuracy: 0.5625\n",
      "Epoch 9/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 1.7560 - accuracy: 0.5625\n",
      "Epoch 10/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 1.5836 - accuracy: 0.6250\n",
      "Epoch 11/512\n",
      "32/32 [==============================] - 0s 990us/sample - loss: 1.5176 - accuracy: 0.7812\n",
      "Epoch 12/512\n",
      "32/32 [==============================] - 0s 764us/sample - loss: 1.4395 - accuracy: 0.7812\n",
      "Epoch 13/512\n",
      "32/32 [==============================] - 0s 929us/sample - loss: 1.3601 - accuracy: 0.8125\n",
      "Epoch 14/512\n",
      "32/32 [==============================] - 0s 886us/sample - loss: 1.2790 - accuracy: 0.8750\n",
      "Epoch 15/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 1.2313 - accuracy: 0.7500\n",
      "Epoch 16/512\n",
      "32/32 [==============================] - 0s 807us/sample - loss: 1.1174 - accuracy: 0.8750\n",
      "Epoch 17/512\n",
      "32/32 [==============================] - 0s 897us/sample - loss: 1.0489 - accuracy: 0.9375\n",
      "Epoch 18/512\n",
      "32/32 [==============================] - 0s 957us/sample - loss: 0.9735 - accuracy: 0.9375\n",
      "Epoch 19/512\n",
      "32/32 [==============================] - 0s 990us/sample - loss: 0.8747 - accuracy: 0.9375\n",
      "Epoch 20/512\n",
      "32/32 [==============================] - 0s 968us/sample - loss: 0.8127 - accuracy: 0.9688\n",
      "Epoch 21/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.7620 - accuracy: 0.9688\n",
      "Epoch 22/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.7543 - accuracy: 1.0000\n",
      "Epoch 23/512\n",
      "32/32 [==============================] - 0s 857us/sample - loss: 0.6732 - accuracy: 0.9688\n",
      "Epoch 24/512\n",
      "32/32 [==============================] - 0s 757us/sample - loss: 0.6217 - accuracy: 1.0000\n",
      "Epoch 25/512\n",
      "32/32 [==============================] - 0s 896us/sample - loss: 0.6006 - accuracy: 1.0000\n",
      "Epoch 26/512\n",
      "32/32 [==============================] - 0s 943us/sample - loss: 0.5190 - accuracy: 1.0000\n",
      "Epoch 27/512\n",
      "32/32 [==============================] - 0s 869us/sample - loss: 0.4888 - accuracy: 0.9688\n",
      "Epoch 28/512\n",
      "32/32 [==============================] - 0s 725us/sample - loss: 0.4679 - accuracy: 1.0000\n",
      "Epoch 29/512\n",
      "32/32 [==============================] - 0s 656us/sample - loss: 0.4122 - accuracy: 0.9688\n",
      "Epoch 30/512\n",
      "32/32 [==============================] - 0s 756us/sample - loss: 0.3863 - accuracy: 1.0000\n",
      "Epoch 31/512\n",
      "32/32 [==============================] - 0s 871us/sample - loss: 0.3640 - accuracy: 1.0000\n",
      "Epoch 32/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.3468 - accuracy: 1.0000\n",
      "Epoch 33/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.3121 - accuracy: 1.0000\n",
      "Epoch 34/512\n",
      "32/32 [==============================] - 0s 855us/sample - loss: 0.2679 - accuracy: 1.0000\n",
      "Epoch 35/512\n",
      "32/32 [==============================] - 0s 974us/sample - loss: 0.2549 - accuracy: 1.0000\n",
      "Epoch 36/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.2351 - accuracy: 1.0000\n",
      "Epoch 37/512\n",
      "32/32 [==============================] - 0s 941us/sample - loss: 0.2321 - accuracy: 1.0000\n",
      "Epoch 38/512\n",
      "32/32 [==============================] - 0s 939us/sample - loss: 0.2343 - accuracy: 1.0000\n",
      "Epoch 39/512\n",
      "32/32 [==============================] - 0s 838us/sample - loss: 0.2014 - accuracy: 1.0000\n",
      "Epoch 40/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.2009 - accuracy: 1.0000\n",
      "Epoch 41/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1560 - accuracy: 1.0000\n",
      "Epoch 42/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1667 - accuracy: 1.0000\n",
      "Epoch 43/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1801 - accuracy: 1.0000\n",
      "Epoch 44/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1385 - accuracy: 1.0000\n",
      "Epoch 45/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1373 - accuracy: 1.0000\n",
      "Epoch 46/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1400 - accuracy: 1.0000\n",
      "Epoch 47/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1470 - accuracy: 1.0000\n",
      "Epoch 48/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1223 - accuracy: 1.0000\n",
      "Epoch 49/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1167 - accuracy: 1.0000\n",
      "Epoch 50/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1162 - accuracy: 1.0000\n",
      "Epoch 51/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1058 - accuracy: 1.0000\n",
      "Epoch 52/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1011 - accuracy: 1.0000\n",
      "Epoch 53/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.1086 - accuracy: 1.0000\n",
      "Epoch 54/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0978 - accuracy: 1.0000\n",
      "Epoch 55/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0932 - accuracy: 1.0000\n",
      "Epoch 56/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0791 - accuracy: 1.0000\n",
      "Epoch 57/512\n",
      "32/32 [==============================] - 0s 2ms/sample - loss: 0.0854 - accuracy: 1.0000\n",
      "Epoch 58/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0716 - accuracy: 1.0000\n",
      "Epoch 59/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0743 - accuracy: 1.0000\n",
      "Epoch 60/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0761 - accuracy: 1.0000\n",
      "Epoch 61/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0812 - accuracy: 1.0000\n",
      "Epoch 62/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0715 - accuracy: 1.0000\n",
      "Epoch 63/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0726 - accuracy: 1.0000\n",
      "Epoch 64/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0723 - accuracy: 1.0000\n",
      "Epoch 65/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0672 - accuracy: 1.0000\n",
      "Epoch 66/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0584 - accuracy: 1.0000\n",
      "Epoch 67/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0609 - accuracy: 1.0000\n",
      "Epoch 68/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0565 - accuracy: 1.0000\n",
      "Epoch 69/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0661 - accuracy: 1.0000\n",
      "Epoch 70/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0481 - accuracy: 1.0000\n",
      "Epoch 71/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0615 - accuracy: 1.0000\n",
      "Epoch 72/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0538 - accuracy: 1.0000\n",
      "Epoch 73/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0490 - accuracy: 1.0000\n",
      "Epoch 74/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0480 - accuracy: 1.0000\n",
      "Epoch 75/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0486 - accuracy: 1.0000\n",
      "Epoch 76/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0415 - accuracy: 1.0000\n",
      "Epoch 77/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0443 - accuracy: 1.0000\n",
      "Epoch 78/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0430 - accuracy: 1.0000\n",
      "Epoch 79/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0410 - accuracy: 1.0000\n",
      "Epoch 80/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0432 - accuracy: 1.0000\n",
      "Epoch 81/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0420 - accuracy: 1.0000\n",
      "Epoch 82/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0421 - accuracy: 1.0000\n",
      "Epoch 83/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0397 - accuracy: 1.0000\n",
      "Epoch 84/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0379 - accuracy: 1.0000\n",
      "Epoch 85/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0342 - accuracy: 1.0000\n",
      "Epoch 86/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0398 - accuracy: 1.0000\n",
      "Epoch 87/512\n",
      "32/32 [==============================] - 0s 920us/sample - loss: 0.0375 - accuracy: 1.0000\n",
      "Epoch 88/512\n",
      "32/32 [==============================] - 0s 886us/sample - loss: 0.0373 - accuracy: 1.0000\n",
      "Epoch 89/512\n",
      "32/32 [==============================] - 0s 879us/sample - loss: 0.0359 - accuracy: 1.0000\n",
      "Epoch 90/512\n",
      "32/32 [==============================] - 0s 833us/sample - loss: 0.0346 - accuracy: 1.0000\n",
      "Epoch 91/512\n",
      "32/32 [==============================] - 0s 943us/sample - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 92/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0285 - accuracy: 1.0000\n",
      "Epoch 93/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0363 - accuracy: 1.0000\n",
      "Epoch 94/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0377 - accuracy: 1.0000\n",
      "Epoch 95/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 96/512\n",
      "32/32 [==============================] - 0s 941us/sample - loss: 0.0284 - accuracy: 1.0000\n",
      "Epoch 97/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0316 - accuracy: 1.0000\n",
      "Epoch 98/512\n",
      "32/32 [==============================] - 0s 910us/sample - loss: 0.0311 - accuracy: 1.0000\n",
      "Epoch 99/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0272 - accuracy: 1.0000\n",
      "Epoch 100/512\n",
      "32/32 [==============================] - 0s 864us/sample - loss: 0.0286 - accuracy: 1.0000\n",
      "Epoch 101/512\n",
      "32/32 [==============================] - 0s 991us/sample - loss: 0.0248 - accuracy: 1.0000\n",
      "Epoch 102/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0283 - accuracy: 1.0000\n",
      "Epoch 103/512\n",
      "32/32 [==============================] - 0s 885us/sample - loss: 0.0294 - accuracy: 1.0000\n",
      "Epoch 104/512\n",
      "32/32 [==============================] - 0s 935us/sample - loss: 0.0271 - accuracy: 1.0000\n",
      "Epoch 105/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0241 - accuracy: 1.0000\n",
      "Epoch 106/512\n",
      "32/32 [==============================] - 0s 933us/sample - loss: 0.0298 - accuracy: 1.0000\n",
      "Epoch 107/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0221 - accuracy: 1.0000\n",
      "Epoch 108/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0226 - accuracy: 1.0000\n",
      "Epoch 109/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0216 - accuracy: 1.0000\n",
      "Epoch 110/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 111/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0239 - accuracy: 1.0000\n",
      "Epoch 112/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0214 - accuracy: 1.0000\n",
      "Epoch 113/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0211 - accuracy: 1.0000\n",
      "Epoch 114/512\n",
      "32/32 [==============================] - 0s 952us/sample - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 115/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0251 - accuracy: 1.0000\n",
      "Epoch 116/512\n",
      "32/32 [==============================] - 0s 976us/sample - loss: 0.0198 - accuracy: 1.0000\n",
      "Epoch 117/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0233 - accuracy: 1.0000\n",
      "Epoch 118/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0218 - accuracy: 1.0000\n",
      "Epoch 119/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 120/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0213 - accuracy: 1.0000\n",
      "Epoch 121/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0203 - accuracy: 1.0000\n",
      "Epoch 122/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0182 - accuracy: 1.0000\n",
      "Epoch 123/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0206 - accuracy: 1.0000\n",
      "Epoch 124/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 125/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 126/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0191 - accuracy: 1.0000\n",
      "Epoch 127/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0202 - accuracy: 1.0000\n",
      "Epoch 128/512\n",
      "32/32 [==============================] - 0s 2ms/sample - loss: 0.0169 - accuracy: 1.0000\n",
      "Epoch 129/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 130/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0196 - accuracy: 1.0000\n",
      "Epoch 131/512\n",
      "32/32 [==============================] - 0s 970us/sample - loss: 0.0185 - accuracy: 1.0000\n",
      "Epoch 132/512\n",
      "32/32 [==============================] - 0s 850us/sample - loss: 0.0193 - accuracy: 1.0000\n",
      "Epoch 133/512\n",
      "32/32 [==============================] - 0s 678us/sample - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 134/512\n",
      "32/32 [==============================] - 0s 758us/sample - loss: 0.0162 - accuracy: 1.0000\n",
      "Epoch 135/512\n",
      "32/32 [==============================] - 0s 871us/sample - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 136/512\n",
      "32/32 [==============================] - 0s 759us/sample - loss: 0.0184 - accuracy: 1.0000\n",
      "Epoch 137/512\n",
      "32/32 [==============================] - 0s 816us/sample - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 138/512\n",
      "32/32 [==============================] - 0s 886us/sample - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 139/512\n",
      "32/32 [==============================] - 0s 736us/sample - loss: 0.0164 - accuracy: 1.0000\n",
      "Epoch 140/512\n",
      "32/32 [==============================] - 0s 864us/sample - loss: 0.0172 - accuracy: 1.0000\n",
      "Epoch 141/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 142/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0170 - accuracy: 1.0000\n",
      "Epoch 143/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0152 - accuracy: 1.0000\n",
      "Epoch 144/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0166 - accuracy: 1.0000\n",
      "Epoch 145/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0171 - accuracy: 1.0000\n",
      "Epoch 146/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0156 - accuracy: 1.0000\n",
      "Epoch 147/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0181 - accuracy: 1.0000\n",
      "Epoch 148/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0148 - accuracy: 1.0000\n",
      "Epoch 149/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0157 - accuracy: 1.0000\n",
      "Epoch 150/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0123 - accuracy: 1.0000\n",
      "Epoch 151/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 152/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0158 - accuracy: 1.0000\n",
      "Epoch 153/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 154/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 155/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0147 - accuracy: 1.0000\n",
      "Epoch 156/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 157/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0141 - accuracy: 1.0000\n",
      "Epoch 158/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 159/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0132 - accuracy: 1.0000\n",
      "Epoch 160/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 161/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 162/512\n",
      "32/32 [==============================] - 0s 991us/sample - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 163/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 164/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0100 - accuracy: 1.0000\n",
      "Epoch 165/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 166/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0137 - accuracy: 1.0000\n",
      "Epoch 167/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 168/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0116 - accuracy: 1.0000\n",
      "Epoch 169/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0136 - accuracy: 1.0000\n",
      "Epoch 170/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 171/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0120 - accuracy: 1.0000\n",
      "Epoch 172/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 173/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0121 - accuracy: 1.0000\n",
      "Epoch 174/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0131 - accuracy: 1.0000\n",
      "Epoch 175/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 176/512\n",
      "32/32 [==============================] - 0s 1000us/sample - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 177/512\n",
      "32/32 [==============================] - 0s 916us/sample - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 178/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0112 - accuracy: 1.0000\n",
      "Epoch 179/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 180/512\n",
      "32/32 [==============================] - 0s 996us/sample - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 181/512\n",
      "32/32 [==============================] - 0s 793us/sample - loss: 0.0101 - accuracy: 1.0000\n",
      "Epoch 182/512\n",
      "32/32 [==============================] - 0s 702us/sample - loss: 0.0097 - accuracy: 1.0000\n",
      "Epoch 183/512\n",
      "32/32 [==============================] - 0s 869us/sample - loss: 0.0110 - accuracy: 1.0000\n",
      "Epoch 184/512\n",
      "32/32 [==============================] - 0s 657us/sample - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 185/512\n",
      "32/32 [==============================] - 0s 656us/sample - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 186/512\n",
      "32/32 [==============================] - 0s 585us/sample - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 187/512\n",
      "32/32 [==============================] - 0s 697us/sample - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 188/512\n",
      "32/32 [==============================] - 0s 658us/sample - loss: 0.0094 - accuracy: 1.0000\n",
      "Epoch 189/512\n",
      "32/32 [==============================] - 0s 648us/sample - loss: 0.0106 - accuracy: 1.0000\n",
      "Epoch 190/512\n",
      "32/32 [==============================] - 0s 607us/sample - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 191/512\n",
      "32/32 [==============================] - 0s 553us/sample - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 192/512\n",
      "32/32 [==============================] - 0s 603us/sample - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 193/512\n",
      "32/32 [==============================] - 0s 527us/sample - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 194/512\n",
      "32/32 [==============================] - 0s 581us/sample - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 195/512\n",
      "32/32 [==============================] - 0s 628us/sample - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 196/512\n",
      "32/32 [==============================] - 0s 716us/sample - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 197/512\n",
      "32/32 [==============================] - 0s 675us/sample - loss: 0.0103 - accuracy: 1.0000\n",
      "Epoch 198/512\n",
      "32/32 [==============================] - 0s 597us/sample - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 199/512\n",
      "32/32 [==============================] - 0s 641us/sample - loss: 0.0113 - accuracy: 1.0000\n",
      "Epoch 200/512\n",
      "32/32 [==============================] - 0s 786us/sample - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 201/512\n",
      "32/32 [==============================] - 0s 770us/sample - loss: 0.0105 - accuracy: 1.0000\n",
      "Epoch 202/512\n",
      "32/32 [==============================] - 0s 843us/sample - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 203/512\n",
      "32/32 [==============================] - 0s 738us/sample - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 204/512\n",
      "32/32 [==============================] - 0s 827us/sample - loss: 0.0098 - accuracy: 1.0000\n",
      "Epoch 205/512\n",
      "32/32 [==============================] - 0s 720us/sample - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 206/512\n",
      "32/32 [==============================] - 0s 899us/sample - loss: 0.0102 - accuracy: 1.0000\n",
      "Epoch 207/512\n",
      "32/32 [==============================] - 0s 554us/sample - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 208/512\n",
      "32/32 [==============================] - 0s 592us/sample - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 209/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0095 - accuracy: 1.0000\n",
      "Epoch 210/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 211/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 212/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 213/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 214/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 215/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 216/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 217/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0088 - accuracy: 1.0000\n",
      "Epoch 218/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 219/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 220/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 221/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 222/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 223/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 224/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 225/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 226/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0083 - accuracy: 1.0000\n",
      "Epoch 227/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 228/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 229/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 230/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 231/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0078 - accuracy: 1.0000\n",
      "Epoch 232/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 233/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 234/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0087 - accuracy: 1.0000\n",
      "Epoch 235/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 236/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 237/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 238/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 239/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0085 - accuracy: 1.0000\n",
      "Epoch 240/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 241/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 242/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 243/512\n",
      "32/32 [==============================] - 0s 886us/sample - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 244/512\n",
      "32/32 [==============================] - 0s 803us/sample - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 245/512\n",
      "32/32 [==============================] - 0s 695us/sample - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 246/512\n",
      "32/32 [==============================] - 0s 621us/sample - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 247/512\n",
      "32/32 [==============================] - 0s 704us/sample - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 248/512\n",
      "32/32 [==============================] - 0s 665us/sample - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 249/512\n",
      "32/32 [==============================] - 0s 621us/sample - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 250/512\n",
      "32/32 [==============================] - 0s 549us/sample - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 251/512\n",
      "32/32 [==============================] - 0s 526us/sample - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 252/512\n",
      "32/32 [==============================] - 0s 804us/sample - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 253/512\n",
      "32/32 [==============================] - 0s 2ms/sample - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 254/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 255/512\n",
      "32/32 [==============================] - 0s 733us/sample - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 256/512\n",
      "32/32 [==============================] - 0s 536us/sample - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 257/512\n",
      "32/32 [==============================] - 0s 733us/sample - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 258/512\n",
      "32/32 [==============================] - 0s 499us/sample - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 259/512\n",
      "32/32 [==============================] - 0s 607us/sample - loss: 0.0068 - accuracy: 1.0000\n",
      "Epoch 260/512\n",
      "32/32 [==============================] - 0s 842us/sample - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 261/512\n",
      "32/32 [==============================] - 0s 972us/sample - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 262/512\n",
      "32/32 [==============================] - 0s 925us/sample - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 263/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 264/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 265/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0076 - accuracy: 1.0000\n",
      "Epoch 266/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0066 - accuracy: 1.0000\n",
      "Epoch 267/512\n",
      "32/32 [==============================] - 0s 961us/sample - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 268/512\n",
      "32/32 [==============================] - 0s 926us/sample - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 269/512\n",
      "32/32 [==============================] - 0s 916us/sample - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 270/512\n",
      "32/32 [==============================] - 0s 927us/sample - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 271/512\n",
      "32/32 [==============================] - 0s 861us/sample - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 272/512\n",
      "32/32 [==============================] - 0s 939us/sample - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 273/512\n",
      "32/32 [==============================] - 0s 725us/sample - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 274/512\n",
      "32/32 [==============================] - 0s 744us/sample - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 275/512\n",
      "32/32 [==============================] - 0s 885us/sample - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 276/512\n",
      "32/32 [==============================] - 0s 990us/sample - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 277/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 278/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 279/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 280/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 281/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 282/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0059 - accuracy: 1.0000\n",
      "Epoch 283/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 284/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 285/512\n",
      "32/32 [==============================] - 0s 2ms/sample - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 286/512\n",
      "32/32 [==============================] - 0s 735us/sample - loss: 0.0070 - accuracy: 1.0000\n",
      "Epoch 287/512\n",
      "32/32 [==============================] - 0s 939us/sample - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 288/512\n",
      "32/32 [==============================] - 0s 683us/sample - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 289/512\n",
      "32/32 [==============================] - 0s 699us/sample - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 290/512\n",
      "32/32 [==============================] - 0s 827us/sample - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 291/512\n",
      "32/32 [==============================] - 0s 976us/sample - loss: 0.0067 - accuracy: 1.0000\n",
      "Epoch 292/512\n",
      "32/32 [==============================] - 0s 1000us/sample - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 293/512\n",
      "32/32 [==============================] - 0s 970us/sample - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 294/512\n",
      "32/32 [==============================] - 0s 702us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 295/512\n",
      "32/32 [==============================] - 0s 707us/sample - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 296/512\n",
      "32/32 [==============================] - 0s 690us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 297/512\n",
      "32/32 [==============================] - 0s 584us/sample - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 298/512\n",
      "32/32 [==============================] - 0s 725us/sample - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 299/512\n",
      "32/32 [==============================] - 0s 699us/sample - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 300/512\n",
      "32/32 [==============================] - 0s 761us/sample - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 301/512\n",
      "32/32 [==============================] - 0s 698us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 302/512\n",
      "32/32 [==============================] - 0s 785us/sample - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 303/512\n",
      "32/32 [==============================] - 0s 854us/sample - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 304/512\n",
      "32/32 [==============================] - 0s 908us/sample - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 305/512\n",
      "32/32 [==============================] - 0s 801us/sample - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 306/512\n",
      "32/32 [==============================] - 0s 821us/sample - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 307/512\n",
      "32/32 [==============================] - 0s 947us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 308/512\n",
      "32/32 [==============================] - 0s 795us/sample - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 309/512\n",
      "32/32 [==============================] - 0s 716us/sample - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 310/512\n",
      "32/32 [==============================] - 0s 589us/sample - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 311/512\n",
      "32/32 [==============================] - 0s 568us/sample - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 312/512\n",
      "32/32 [==============================] - 0s 580us/sample - loss: 0.0048 - accuracy: 1.0000\n",
      "Epoch 313/512\n",
      "32/32 [==============================] - 0s 516us/sample - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 314/512\n",
      "32/32 [==============================] - 0s 543us/sample - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 315/512\n",
      "32/32 [==============================] - 0s 546us/sample - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 316/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 441us/sample - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 317/512\n",
      "32/32 [==============================] - 0s 486us/sample - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 318/512\n",
      "32/32 [==============================] - 0s 457us/sample - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 319/512\n",
      "32/32 [==============================] - 0s 502us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 320/512\n",
      "32/32 [==============================] - 0s 511us/sample - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 321/512\n",
      "32/32 [==============================] - 0s 539us/sample - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 322/512\n",
      "32/32 [==============================] - 0s 514us/sample - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 323/512\n",
      "32/32 [==============================] - 0s 500us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 324/512\n",
      "32/32 [==============================] - 0s 500us/sample - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 325/512\n",
      "32/32 [==============================] - 0s 492us/sample - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 326/512\n",
      "32/32 [==============================] - 0s 563us/sample - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 327/512\n",
      "32/32 [==============================] - 0s 506us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 328/512\n",
      "32/32 [==============================] - 0s 584us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 329/512\n",
      "32/32 [==============================] - 0s 505us/sample - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 330/512\n",
      "32/32 [==============================] - 0s 495us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 331/512\n",
      "32/32 [==============================] - 0s 500us/sample - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 332/512\n",
      "32/32 [==============================] - 0s 522us/sample - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 333/512\n",
      "32/32 [==============================] - 0s 780us/sample - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 334/512\n",
      "32/32 [==============================] - 0s 658us/sample - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 335/512\n",
      "32/32 [==============================] - 0s 628us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 336/512\n",
      "32/32 [==============================] - 0s 733us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 337/512\n",
      "32/32 [==============================] - 0s 712us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 338/512\n",
      "32/32 [==============================] - 0s 562us/sample - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 339/512\n",
      "32/32 [==============================] - 0s 582us/sample - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 340/512\n",
      "32/32 [==============================] - 0s 524us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 341/512\n",
      "32/32 [==============================] - 0s 555us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 342/512\n",
      "32/32 [==============================] - 0s 570us/sample - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 343/512\n",
      "32/32 [==============================] - 0s 520us/sample - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 344/512\n",
      "32/32 [==============================] - 0s 529us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 345/512\n",
      "32/32 [==============================] - 0s 518us/sample - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 346/512\n",
      "32/32 [==============================] - 0s 588us/sample - loss: 0.0046 - accuracy: 1.0000\n",
      "Epoch 347/512\n",
      "32/32 [==============================] - 0s 545us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 348/512\n",
      "32/32 [==============================] - 0s 732us/sample - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 349/512\n",
      "32/32 [==============================] - 0s 570us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 350/512\n",
      "32/32 [==============================] - 0s 610us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 351/512\n",
      "32/32 [==============================] - 0s 738us/sample - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 352/512\n",
      "32/32 [==============================] - 0s 759us/sample - loss: 0.0051 - accuracy: 1.0000\n",
      "Epoch 353/512\n",
      "32/32 [==============================] - 0s 660us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 354/512\n",
      "32/32 [==============================] - 0s 600us/sample - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 355/512\n",
      "32/32 [==============================] - 0s 634us/sample - loss: 0.0052 - accuracy: 1.0000\n",
      "Epoch 356/512\n",
      "32/32 [==============================] - 0s 568us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 357/512\n",
      "32/32 [==============================] - 0s 656us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 358/512\n",
      "32/32 [==============================] - 0s 618us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 359/512\n",
      "32/32 [==============================] - 0s 597us/sample - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 360/512\n",
      "32/32 [==============================] - 0s 672us/sample - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 361/512\n",
      "32/32 [==============================] - 0s 663us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 362/512\n",
      "32/32 [==============================] - 0s 766us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 363/512\n",
      "32/32 [==============================] - 0s 734us/sample - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 364/512\n",
      "32/32 [==============================] - 0s 763us/sample - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 365/512\n",
      "32/32 [==============================] - 0s 709us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 366/512\n",
      "32/32 [==============================] - 0s 741us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 367/512\n",
      "32/32 [==============================] - 0s 669us/sample - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 368/512\n",
      "32/32 [==============================] - 0s 661us/sample - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 369/512\n",
      "32/32 [==============================] - 0s 676us/sample - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 370/512\n",
      "32/32 [==============================] - 0s 618us/sample - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 371/512\n",
      "32/32 [==============================] - 0s 530us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 372/512\n",
      "32/32 [==============================] - 0s 982us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 373/512\n",
      "32/32 [==============================] - 0s 812us/sample - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 374/512\n",
      "32/32 [==============================] - 0s 652us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 375/512\n",
      "32/32 [==============================] - 0s 580us/sample - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 376/512\n",
      "32/32 [==============================] - 0s 875us/sample - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 377/512\n",
      "32/32 [==============================] - 0s 814us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 378/512\n",
      "32/32 [==============================] - 0s 736us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 379/512\n",
      "32/32 [==============================] - 0s 848us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 380/512\n",
      "32/32 [==============================] - 0s 974us/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 381/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 382/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 383/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 384/512\n",
      "32/32 [==============================] - 0s 972us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 385/512\n",
      "32/32 [==============================] - 0s 979us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 386/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 387/512\n",
      "32/32 [==============================] - 0s 773us/sample - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 388/512\n",
      "32/32 [==============================] - 0s 683us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 389/512\n",
      "32/32 [==============================] - 0s 615us/sample - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 390/512\n",
      "32/32 [==============================] - 0s 815us/sample - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 391/512\n",
      "32/32 [==============================] - 0s 822us/sample - loss: 0.0041 - accuracy: 1.0000\n",
      "Epoch 392/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 393/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 394/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 917us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 395/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 396/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 397/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 398/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 399/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 400/512\n",
      "32/32 [==============================] - 0s 972us/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 401/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 402/512\n",
      "32/32 [==============================] - 0s 985us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 403/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 404/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 405/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 406/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 407/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 408/512\n",
      "32/32 [==============================] - 0s 966us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 409/512\n",
      "32/32 [==============================] - 0s 828us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 410/512\n",
      "32/32 [==============================] - 0s 656us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 411/512\n",
      "32/32 [==============================] - 0s 615us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 412/512\n",
      "32/32 [==============================] - 0s 702us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 413/512\n",
      "32/32 [==============================] - 0s 577us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 414/512\n",
      "32/32 [==============================] - 0s 535us/sample - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 415/512\n",
      "32/32 [==============================] - 0s 522us/sample - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 416/512\n",
      "32/32 [==============================] - 0s 536us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 417/512\n",
      "32/32 [==============================] - 0s 575us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 418/512\n",
      "32/32 [==============================] - 0s 560us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 419/512\n",
      "32/32 [==============================] - 0s 501us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 420/512\n",
      "32/32 [==============================] - 0s 471us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 421/512\n",
      "32/32 [==============================] - 0s 517us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 422/512\n",
      "32/32 [==============================] - 0s 556us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 423/512\n",
      "32/32 [==============================] - 0s 553us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 424/512\n",
      "32/32 [==============================] - 0s 513us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 425/512\n",
      "32/32 [==============================] - 0s 531us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 426/512\n",
      "32/32 [==============================] - 0s 503us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 427/512\n",
      "32/32 [==============================] - 0s 519us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 428/512\n",
      "32/32 [==============================] - 0s 543us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 429/512\n",
      "32/32 [==============================] - 0s 494us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 430/512\n",
      "32/32 [==============================] - 0s 512us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 431/512\n",
      "32/32 [==============================] - 0s 529us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 432/512\n",
      "32/32 [==============================] - 0s 607us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 433/512\n",
      "32/32 [==============================] - 0s 623us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 434/512\n",
      "32/32 [==============================] - 0s 645us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 435/512\n",
      "32/32 [==============================] - 0s 513us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 436/512\n",
      "32/32 [==============================] - 0s 499us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 437/512\n",
      "32/32 [==============================] - 0s 534us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 438/512\n",
      "32/32 [==============================] - 0s 524us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 439/512\n",
      "32/32 [==============================] - 0s 501us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 440/512\n",
      "32/32 [==============================] - 0s 564us/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 441/512\n",
      "32/32 [==============================] - 0s 539us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 442/512\n",
      "32/32 [==============================] - 0s 686us/sample - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 443/512\n",
      "32/32 [==============================] - 0s 644us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 444/512\n",
      "32/32 [==============================] - 0s 644us/sample - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 445/512\n",
      "32/32 [==============================] - 0s 634us/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 446/512\n",
      "32/32 [==============================] - 0s 636us/sample - loss: 0.0034 - accuracy: 1.0000\n",
      "Epoch 447/512\n",
      "32/32 [==============================] - 0s 573us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 448/512\n",
      "32/32 [==============================] - 0s 670us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 449/512\n",
      "32/32 [==============================] - 0s 706us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 450/512\n",
      "32/32 [==============================] - 0s 673us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 451/512\n",
      "32/32 [==============================] - 0s 651us/sample - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 452/512\n",
      "32/32 [==============================] - 0s 708us/sample - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 453/512\n",
      "32/32 [==============================] - 0s 621us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 454/512\n",
      "32/32 [==============================] - 0s 641us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 455/512\n",
      "32/32 [==============================] - 0s 707us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 456/512\n",
      "32/32 [==============================] - 0s 555us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 457/512\n",
      "32/32 [==============================] - 0s 550us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 458/512\n",
      "32/32 [==============================] - 0s 828us/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 459/512\n",
      "32/32 [==============================] - 0s 882us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 460/512\n",
      "32/32 [==============================] - 0s 775us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 461/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 462/512\n",
      "32/32 [==============================] - 0s 861us/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 463/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 464/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 465/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 466/512\n",
      "32/32 [==============================] - 0s 940us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 467/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 468/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 469/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 470/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 471/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 472/512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 473/512\n",
      "32/32 [==============================] - 0s 970us/sample - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 474/512\n",
      "32/32 [==============================] - 0s 1ms/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 475/512\n",
      "32/32 [==============================] - 0s 866us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 476/512\n",
      "32/32 [==============================] - 0s 829us/sample - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 477/512\n",
      "32/32 [==============================] - 0s 680us/sample - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 478/512\n",
      "32/32 [==============================] - 0s 711us/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 479/512\n",
      "32/32 [==============================] - 0s 607us/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 480/512\n",
      "32/32 [==============================] - 0s 598us/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 481/512\n",
      "32/32 [==============================] - 0s 554us/sample - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 482/512\n",
      "32/32 [==============================] - 0s 628us/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 483/512\n",
      "32/32 [==============================] - 0s 568us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 484/512\n",
      "32/32 [==============================] - 0s 523us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 485/512\n",
      "32/32 [==============================] - 0s 543us/sample - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 486/512\n",
      "32/32 [==============================] - 0s 531us/sample - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 487/512\n",
      "32/32 [==============================] - 0s 506us/sample - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 488/512\n",
      "32/32 [==============================] - 0s 519us/sample - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 489/512\n",
      "32/32 [==============================] - 0s 513us/sample - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 490/512\n",
      "32/32 [==============================] - 0s 515us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 491/512\n",
      "32/32 [==============================] - 0s 521us/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 492/512\n",
      "32/32 [==============================] - 0s 575us/sample - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 493/512\n",
      "32/32 [==============================] - 0s 531us/sample - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 494/512\n",
      "32/32 [==============================] - 0s 545us/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 495/512\n",
      "32/32 [==============================] - 0s 578us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 496/512\n",
      "32/32 [==============================] - 0s 544us/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 497/512\n",
      "32/32 [==============================] - 0s 487us/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 498/512\n",
      "32/32 [==============================] - 0s 505us/sample - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 499/512\n",
      "32/32 [==============================] - 0s 537us/sample - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 500/512\n",
      "32/32 [==============================] - 0s 555us/sample - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 501/512\n",
      "32/32 [==============================] - 0s 520us/sample - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 502/512\n",
      "32/32 [==============================] - 0s 558us/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 503/512\n",
      "32/32 [==============================] - 0s 545us/sample - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 504/512\n",
      "32/32 [==============================] - 0s 517us/sample - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 505/512\n",
      "32/32 [==============================] - 0s 537us/sample - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 506/512\n",
      "32/32 [==============================] - 0s 566us/sample - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 507/512\n",
      "32/32 [==============================] - 0s 472us/sample - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 508/512\n",
      "32/32 [==============================] - 0s 499us/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 509/512\n",
      "32/32 [==============================] - 0s 523us/sample - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 510/512\n",
      "32/32 [==============================] - 0s 489us/sample - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 511/512\n",
      "32/32 [==============================] - 0s 487us/sample - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 512/512\n",
      "32/32 [==============================] - 0s 600us/sample - loss: 0.0021 - accuracy: 1.0000\n",
      "10000/1 - 1s - loss: 1.3171 - accuracy: 0.5548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.7233221655845643, 0.5548]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "   \n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'), \n",
    "     data_augumentation, #학습 하면서 random data augmentation\n",
    "    tf.keras.layers.Dense(units=128, activation='relu'), \n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer='SGD', loss=loss_fn, metrics=['accuracy'])\n",
    "model.fit(x_train[0:32], y_train[0:32], batch_size=4, epochs=512)\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "20c17895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             multiple                  100480    \n",
      "_________________________________________________________________\n",
      "sequential_5 (Sequential)    multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             multiple                  16512     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             multiple                  1290      \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000/1 - 1s - loss: 1.3171 - accuracy: 0.5548\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer sequential_16 is incompatible with the layer: expected axis -1 of input shape to have value 784 but received input with shape [None, 28, 28, 1]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_27964\\3996046397.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SGD'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mtest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    831\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    834\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m   def predict(self,\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    454\u001b[0m     return self._model_iteration(\n\u001b[0;32m    455\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 456\u001b[1;33m         sample_weight=sample_weight, steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 396\u001b[1;33m           distribution_strategy=strategy)\n\u001b[0m\u001b[0;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m         steps=steps)\n\u001b[0m\u001b[0;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[0;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2417\u001b[0m     \u001b[1;31m# First, we build the model on the fly if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2419\u001b[1;33m       \u001b[0mall_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_input\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_model_with_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2420\u001b[0m       \u001b[0mis_build_called\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2421\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_build_model_with_inputs\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m   2620\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2621\u001b[0m       \u001b[0mcast_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2622\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2623\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mprocessed_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_dict_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_set_inputs\u001b[1;34m(self, inputs, outputs, training)\u001b[0m\n\u001b[0;32m   2707\u001b[0m           \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2708\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2709\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2710\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2711\u001b[0m         \u001b[1;31m# This Model or a submodel is dynamic and hasn't overridden\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    810\u001b[0m         \u001b[1;31m# are casted, not before.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m         input_spec.assert_input_compatibility(self.input_spec, inputs,\n\u001b[1;32m--> 812\u001b[1;33m                                               self.name)\n\u001b[0m\u001b[0;32m    813\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\myenv\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\input_spec.py\u001b[0m in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 \u001b[1;34m' incompatible with the layer: expected axis '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    212\u001b[0m                 \u001b[1;34m' of input shape to have value '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                 ' but received input with shape ' + str(shape))\n\u001b[0m\u001b[0;32m    214\u001b[0m     \u001b[1;31m# Check shape.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mspec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer sequential_16 is incompatible with the layer: expected axis -1 of input shape to have value 784 but received input with shape [None, 28, 28, 1]"
     ]
    }
   ],
   "source": [
    "#[1.582899118232727, 0.5407] data augmentation 없는 결과...\n",
    "#[1.7467470359802246, 0.5377] guassian noise data augmentatin 결과\n",
    "model.summary()\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "test_model = tf.keras.Sequential() # model의 있는 ?:?까지\n",
    "for layer in model.layers[1:]:\n",
    "    # layer가 0,1,2,3,4 중 1,2,3,4만 add\n",
    "    test_model.add(layer)\n",
    "    \n",
    "test_model.compile(optimizer='SGD', loss=loss_fn, metrics=['accuracy'])\n",
    "test_model.evaluate(x_test, y_test, verbose=2)\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c2fa84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
